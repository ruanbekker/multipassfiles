#cloud-config
groups:
  - mpusers

users:
  - default
  - name: ubuntu
    groups: mpusers
    sudo:  ALL=(ALL) NOPASSWD:ALL
    #ssh_authorized_keys:
    #  - ssh-rsa __DEFAULT__
      
package_update: true

packages:
 - apt-transport-https
 - gnupg
 - ca-certificates
 - wget
 - curl
 - apt-utils
 - net-tools
 - python3.8
 - python3.8-dev
 - python3-pip
 - libffi-dev 
 - libkrb5-dev 
 - libsasl2-dev 
 - libssl-dev 
 - libffi-dev 
 - build-essential 
 - libblas-dev 
 - liblapack-dev 
 - libpq-dev 
 - git
 - apt-utils 
 - curl 
 - rsync 
 - netcat 
 - locales

write_files:
  - content: |
      #!/usr/bin/env bash
      export AIRFLOW_HOME=/usr/local/airflow
      export PATH=$PATH:${AIRFLOW_HOME}/.local/bin
      export AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS=True
      export AIRFLOW__CORE__FERNET_KEY=hpDX91iogoIhCu6YAxJONYA7ltfyVvidVDV1oM6sVVo=
      export AIRFLOW__WEBSERVER__RBAC=True
      export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@192.168.64.38:5432/airflow"
      export AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      export AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@192.168.64.38/airflow
      export AIRFLOW__CELERY__BROKER_URL=redis://192.168.64.39:6379/0
      export AIRFLOW__CORE__LOAD_EXAMPLES=False
    path: /etc/profile.d/airflow.sh
    owner: root:root
    permissions: '0755'

  - content: |
      PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/airflow/.local/bin
      AIRFLOW_HOME=/usr/local/airflow
      AIRFLOW__WEBSERVER__UPDATE_FAB_PERMS=True
      AIRFLOW__CORE__FERNET_KEY=hpDX91iogoIhCu6YAxJONYA7ltfyVvidVDV1oM6sVVo=
      AIRFLOW__WEBSERVER__RBAC=True
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@192.168.64.38:5432/airflow"
      AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@192.168.64.38/airflow
      AIRFLOW__CELERY__BROKER_URL=redis://192.168.64.39:6379/0
      AIRFLOW__CORE__LOAD_EXAMPLES=False
    path: /etc/airflow/env
    owner: root:root
    permissions: '0644'

  - content: |
      [Unit]
      Description=airflow-worker
      Wants=network-online.target
      After=network-online.target
      StartLimitIntervalSec=500
      StartLimitBurst=5

      [Service]
      User=airflow
      Group=airflow
      Restart=on-failure
      RestartSec=5s
      EnvironmentFile=/etc/airflow/env
      WorkingDirectory=/usr/local/airflow
      ExecStart=/usr/local/airflow/.local/bin/airflow celery worker

      [Install]
      WantedBy=multi-user.target
    path: /etc/systemd/system/airflow-worker.service
    owner: root:root
    permissions: '0644'

  - content: |-
      #!/usr/bin/env bash
      set -x
      ln -sf /usr/bin/python3 /usr/bin/python
      sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen
      locale-gen && update-locale
      export LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8
      useradd -ms /bin/bash -d /usr/local/airflow airflow
      mkdir -p /usr/local/airflow/dags
      chown -R airflow:airflow /usr/local/airflow/dags
    path: /opt/scripts/prepare.sh
    owner: root:root
    permissions: '0755'

  - content: |-
      #!/usr/bin/env bash
      export AIRFLOW_VERSION=2.3.0
      export PYTHON_VERSION="$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
      python3 -m pip install --upgrade --no-warn-script-location setuptools wheel testresources toolz
      python3 -m pip install --user --no-warn-script-location apache-airflow[async,aws,celery,crypto,jdbc,postgres,password]==${AIRFLOW_VERSION} --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
      python3 -m pip install --user redis
    path: /opt/scripts/install_airflow.sh
    owner: root:root
    permissions: '0755'

  - content: |-
      from airflow import DAG
      from airflow.operators.bash_operator import BashOperator
      from datetime import datetime, timedelta

      default_args = {
          'owner': 'admin',
          'start_date': datetime(2022, 6, 16),
          'depends_on_past': False,
          'email': ['admin@localhost'],
          'email_on_failure': False,
          'email_on_retry': False,
          'retries': 1,
          'retry_delay': timedelta(minutes=5),
          # 'queue': 'bash_queue',
          # 'pool': 'backfill',
          # 'priority_weight': 10,
          # 'end_date': datetime(2021, 7, 29),
      }

      dag = DAG('bash_example_dag', default_args=default_args, schedule_interval=timedelta(1))

      # t1, t2 and t3 are examples of tasks created by instantiating operators
      t1 = BashOperator(task_id='print_date', bash_command='date', dag=dag)
      t2 = BashOperator(task_id='sleep', bash_command='sleep 5', retries=3, dag=dag)

      templated_command = """
          {% for i in range(5) %}
              echo "{{ ds }}"
              echo "{{ macros.ds_add(ds, 7)}}"
              echo "{{ params.my_param }}"
          {% endfor %}
      """

      t3 = BashOperator(
          task_id="templated",
          bash_command=templated_command,
          params={"my_param": "Parameter I passed in"},
          dag=dag,
      )

      t2.set_upstream(t1)
      t3.set_upstream(t1)
    path: /opt/scripts/bashdag.py
    owner: ubuntu:ubuntu
    permissions: '0644'

bootcmd:
  - mkdir -p /opt/scripts /opt/docs /etc/airflow

runcmd:
  - /opt/scripts/prepare.sh
  - sudo -H -u airflow bash -c '/opt/scripts/install_airflow.sh' >> /tmp/install.log
  - chown -R airflow:airflow /usr/local/airflow
  - cp /opt/scripts/bashdag.py /usr/local/airflow/dags/bashdag.py
  - chown -R airflow:airflow /usr/local/airflow
  - systemctl daemon-reload
